{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "323469b2-58d6-47eb-8bfd-be1827614ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398acb82-5976-46a4-b9eb-ad79c6fccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat function session dengan retry\n",
    "def create_session_with_retries(retries=3, backoff_factor=0.3):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries, \n",
    "        read=retries, \n",
    "        connect=retries, \n",
    "        backoff_factor=backoff_factor, \n",
    "        status_forcelist=[500, 502, 503, 504]  # Retry status HTTP \n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6310e659-093c-45de-a698-82c61fad0be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape artikel dari tiap URL\n",
    "def scrape_article(session, url, keywords):\n",
    "    try:\n",
    "        # Define headers to avoid loading images & iklan\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Cache-Control': 'max-age=0'\n",
    "        }\n",
    "\n",
    "        # Send request dengan session retry & header spesifik\n",
    "        response = session.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    " \n",
    "        # Ekstrak rubrik\n",
    "        rubric = soup.select_one('#wrapforleftpush > div.wrapping.mar-t-10 > div.container-section > div.bag-kiri > div.breadcumb.fs18 > a:nth-child(1)')\n",
    "        rubric_text = rubric.get_text(strip=True) if rubric else 'N/A'\n",
    " \n",
    "        # Ekstrak tanggal\n",
    "        date = soup.select_one('#wrapforleftpush > div.wrapping.mar-t-10 > div.container-section > div.bag-kiri > div.fs14.ff-opensans.font-gray')\n",
    "        date_text = date.get_text(strip=True) if date else 'N/A'\n",
    " \n",
    "        # Ekstrak headline\n",
    "        headline = soup.select_one('#wrapforleftpush > div.wrapping.mar-t-10 > div.container-section > div.bag-kiri > h1')\n",
    "        headline_text = headline.get_text(strip=True) if headline else 'N/A'\n",
    " \n",
    "        # Ekstrak nama reporter dan editor\n",
    "        author_info = soup.select_one('#wrapforleftpush > div.wrapping.mar-t-10 > div.container-section > div.bag-kiri > div.box-det-desk-2 > div.tmpt-desk-kon > p:nth-child(1)')\n",
    "        author_text = author_info.get_text(strip=True) if author_info else 'N/A'\n",
    " \n",
    "        # Ekstrak lead berita\n",
    "        lead = soup.select_one('#wrapforleftpush > div.wrapping.mar-t-10 > div.container-section > div.bag-kiri > div.box-det-desk-2 > div.tmpt-desk-kon > p:nth-child(2)')\n",
    "        lead_text = lead.get_text(strip=True) if lead else 'N/A'\n",
    " \n",
    "        # Ekstrak body berita\n",
    "        body_paragraphs = [\n",
    "            p.get_text(strip=True) for p in [\n",
    "                soup.select_one(f'#wrapforleftpush > div.wrapping.mar-t-10 > div.container-section > div.bag-kiri > div.box-det-desk-2 > div.tmpt-desk-kon > p:nth-child({i})')\n",
    "                for i in [2, 3, 6, 7, 9, 10, 11, 13, 14, 15]\n",
    "            ] if p\n",
    "        ]\n",
    "        body_text = \" \".join(body_paragraphs) if body_paragraphs else 'N/A'\n",
    " \n",
    "        # Cek keywords\n",
    "        content = f\"{headline_text} {lead_text} {body_text}\".lower()\n",
    "        keyword_match = any(keyword.lower() in content for keyword in keywords)\n",
    " \n",
    "        # Return (if the content matches the keywordssss)\n",
    "        if keyword_match:\n",
    "            return {\n",
    "                'URL': url,\n",
    "                'Rubric': rubric_text,\n",
    "                'Date': date_text,\n",
    "                'Headline': headline_text,\n",
    "                'Author': author_text,  # Reporter dan editor\n",
    "                'Lead': lead_text,\n",
    "                'Body': body_text\n",
    "            }\n",
    "        else:\n",
    " \n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b20db00-db49-448a-b9d0-323693835bef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to this file: C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\excel\\scraped_data_incremental.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Progress:   4%|█▊                                               | 12432/347504 [1:47:22<376:57:29,  4.05s/url]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping https://investasi.kontan.co.id/news/gelar-rupslb-mitra-komunikasi-nusantara-mknt-rombak-susunan-direksi-dan-komisaris: HTTPSConnectionPool(host='investasi.kontan.co.id', port=443): Max retries exceeded with url: /news/gelar-rupslb-mitra-komunikasi-nusantara-mknt-rombak-susunan-direksi-dan-komisaris (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000027791ED6990>: Failed to establish a new connection: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Progress:  14%|██████▋                                        | 49569/347504 [10:23:45<2450:37:02, 29.61s/url]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping https://investasi.kontan.co.id/news/kinerja-moncer-sejak-awal-tahun-penghuni-indeks-idxgrowth30-ini-layak-koleksi: HTTPSConnectionPool(host='investasi.kontan.co.id', port=443): Max retries exceeded with url: /news/kinerja-moncer-sejak-awal-tahun-penghuni-indeks-idxgrowth30-ini-layak-koleksi (Caused by ResponseError('too many 504 error responses'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Progress:  23%|███████████▎                                    | 81630/347504 [22:10:40<282:53:27,  3.83s/url]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping https://analisis.kontan.co.id/news/lebih-tegas: HTTPSConnectionPool(host='analisis.kontan.co.id', port=443): Max retries exceeded with url: /news/lebih-tegas (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002779466A490>: Failed to resolve 'analisis.kontan.co.id' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Progress:  23%|███████████▎                                    | 81631/347504 [22:10:42<237:59:59,  3.22s/url]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping https://internasional.kontan.co.id/news/pbb-misil-yang-digunakan-untuk-menyerang-arab-saudi-berasal-dari-iran: HTTPSConnectionPool(host='internasional.kontan.co.id', port=443): Max retries exceeded with url: /news/pbb-misil-yang-digunakan-untuk-menyerang-arab-saudi-berasal-dari-iran (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000277961FA490>: Failed to resolve 'internasional.kontan.co.id' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Progress:  23%|███████████▎                                    | 81632/347504 [22:10:44<206:35:29,  2.80s/url]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping https://investasi.kontan.co.id/news/ihsg-ditutup-melemah-047-ke-4831-pada-sesi-i-sektor-aneka-industri-ke-zona-hijau: HTTPSConnectionPool(host='investasi.kontan.co.id', port=443): Max retries exceeded with url: /news/ihsg-ditutup-melemah-047-ke-4831-pada-sesi-i-sektor-aneka-industri-ke-zona-hijau (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027797AA1450>: Failed to resolve 'investasi.kontan.co.id' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Progress:  23%|███████████▎                                    | 81633/347504 [22:10:45<184:34:47,  2.50s/url]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping https://investasi.kontan.co.id/news/rupiah-ada-di-level-rp-14204-per-dolar-as-melemah-129-pada-siang-hari-ini: HTTPSConnectionPool(host='investasi.kontan.co.id', port=443): Max retries exceeded with url: /news/rupiah-ada-di-level-rp-14204-per-dolar-as-melemah-129-pada-siang-hari-ini (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027797AA16D0>: Failed to resolve 'investasi.kontan.co.id' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Progress:  23%|███████████▎                                    | 81634/347504 [22:10:47<169:11:34,  2.29s/url]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping https://industri.kontan.co.id/news/inilah-jajaran-direksi-pertamina-yang-baru-ada-mantan-direksi-mandiri-sekuritas: HTTPSConnectionPool(host='industri.kontan.co.id', port=443): Max retries exceeded with url: /news/inilah-jajaran-direksi-pertamina-yang-baru-ada-mantan-direksi-mandiri-sekuritas (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027797AA2850>: Failed to resolve 'industri.kontan.co.id' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Progress:  23%|███████████▎                                    | 81635/347504 [22:10:49<158:25:27,  2.15s/url]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping https://nasional.kontan.co.id/news/agar-pbi-tepat-sasaran-penyempurnaan-dtks-harus-terus-dilakukan: HTTPSConnectionPool(host='nasional.kontan.co.id', port=443): Max retries exceeded with url: /news/agar-pbi-tepat-sasaran-penyempurnaan-dtks-harus-terus-dilakukan (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027797AA20D0>: Failed to resolve 'nasional.kontan.co.id' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Progress:  23%|███████████▎                                    | 81636/347504 [22:10:51<150:53:06,  2.04s/url]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping https://industri.kontan.co.id/news/rampingkan-jajaran-direksi-ini-susunan-direksi-baru-pertamina: HTTPSConnectionPool(host='industri.kontan.co.id', port=443): Max retries exceeded with url: /news/rampingkan-jajaran-direksi-ini-susunan-direksi-baru-pertamina (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027797AA3390>: Failed to resolve 'industri.kontan.co.id' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Progress:  26%|████████████▍                                   | 90385/347504 [26:42:16<119:07:20,  1.67s/url]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load URLs from file\n",
    "url_file_path = r'C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\excel\\(1jan19_30jun24) scraped_urls filtered.xlsx'\n",
    "df_urls = pd.read_excel(url_file_path)\n",
    "\n",
    "# Define fixed output file path (no timestamp)\n",
    "output_file_path = r'C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\excel\\scraped_data_incremental.xlsx'\n",
    "print(f\"Saving to this file: {output_file_path}\")\n",
    "\n",
    "# Create DataFrame for scraped data\n",
    "if os.path.exists(output_file_path):\n",
    "    df_scraped = pd.read_excel(output_file_path)\n",
    "    scraped_urls = set(df_scraped['URL'].tolist())  # Avoid duplicates\n",
    "else:\n",
    "    df_scraped = pd.DataFrame(columns=['URL', 'Rubric', 'Date', 'Headline', 'Author', 'Lead', 'Body'])\n",
    "    scraped_urls = set()\n",
    "\n",
    "# Define keywords\n",
    "keywords = [\n",
    "    'pendapatan masyarakat', 'penghasilan masyarakat', 'ekspektasi pendapatan', 'gaji', 'upah', 'gaji karyawan', 'upah karyawan',\n",
    "    'pendapatan riil', 'tabungan masyarakat', 'konsumsi masyarakat', 'lapangan kerja', 'lapangan pekerjaan', 'pengangguran',\n",
    "    'lowongan kerja', 'PHK', 'Pemutusan Hubungan Kerja', 'kesempatan kerja', 'penyerapan tenaga kerja', 'pengangguran terbuka',\n",
    "    'pengangguran terselubung', 'pengangguran struktural', 'kegiatan usaha', 'aktivitas bisnis', 'prospek usaha', 'inovasi bisnis',\n",
    "    'optimisme konsumen', 'UMKM', 'pembelian barang tahan lama', 'daya beli', 'belanja konsumen', 'barang tahan lama',\n",
    "    'harga barang elektronik', 'harga produk elektronik', 'harga mobil', 'harga produk tahan lama', 'durable goods',\n",
    "    'pertumbuhan ekonomi', 'PDB', 'ekspansi ekonomi', 'kontraksi ekonomi', 'resesi ekonomi', 'krisis ekonomi', 'pemulihan ekonomi',\n",
    "    'Pemulihan Ekonomi Nasional', 'economic recovery', 'laju ekonomi', 'inflasi', 'deflasi', 'harga barang komoditas',\n",
    "    'harga pangan', 'harga energi', 'harga BBM', 'nilai tukar', 'depresiasi mata uang', 'inflasi inti', 'tingkat inflasi',\n",
    "    'tekanan inflasi', 'Indeks Harga Konsumen', 'harga barang pokok', 'Indeks Harga Produsen', 'permintaan agregat',\n",
    "    'kebijakan moneter', 'suku bunga Bank Indonesia', 'suku bunga BI', 'BI rate', 'rupiah', 'stimulus moneter', 'suku bunga acuan',\n",
    "    'suku bunga kredit', 'suku bunga pinjaman', 'suku bunga deposito', 'suku bunga pasar', 'FOMC Rate', 'suku bunga rendah',\n",
    "    'suku bunga tinggi', 'tingkat bunga pinjaman', 'bunga kredit perbankan', 'yield obligasi', 'likuiditas perbankan', 'pasar obligasi'\n",
    "]\n",
    "\n",
    "# Create session with retry\n",
    "session = create_session_with_retries()\n",
    "\n",
    "# Start scraping loop\n",
    "for url in tqdm(df_urls['url'], desc='Scraping Progress', unit='url'):\n",
    "    if url in scraped_urls:\n",
    "        continue  # Skip already scraped\n",
    "\n",
    "    article_data = scrape_article(session, url, keywords)\n",
    "    if article_data:\n",
    "        # Append data\n",
    "        df_scraped = pd.concat([df_scraped, pd.DataFrame([article_data])], ignore_index=True)\n",
    "\n",
    "        # Save after every successful scrape\n",
    "        try:\n",
    "            df_scraped.to_excel(output_file_path, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to save data for {url}: {e}\")\n",
    "\n",
    "        scraped_urls.add(url)\n",
    "\n",
    "print(f\"✅ Scraping completed. Final data saved to: {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
